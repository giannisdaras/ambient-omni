{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb81093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "import psutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import dnnlib\n",
    "from torch_utils import distributed as dist\n",
    "from torch_utils import training_stats\n",
    "from torch_utils import persistence\n",
    "from torch_utils import misc\n",
    "import wandb\n",
    "import ambient_utils\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcff784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calculate_metrics_quality import load_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01270cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def view_mosaic(images, nrow=5):\n",
    "  mosaic = torchvision.utils.make_grid([x for x in images], nrow=min(nrow, len(images)))\n",
    "  mosaic = torchvision.transforms.ToPILImage()(mosaic)\n",
    "  display(mosaic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kwargs = dnnlib.EasyDict(\n",
    "  path='../datasets/img512.zip', \n",
    "  use_labels=True, \n",
    "  corruption_probability=0.0,\n",
    "  normalize=False,\n",
    "  use_other_keys=False\n",
    ")\n",
    "\n",
    "annotations_qualities_path = \"../annotations/clip_iqa_patch_average.pkl\"\n",
    "bad_data_percentage = 0.8\n",
    "bad_data_sigma_min = 0.2\n",
    "use_ambient_crops = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc067e2b",
   "metadata": {},
   "source": [
    "## Verify qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_qualities = load_stats(annotations_qualities_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07063737",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_qualities['CLIP-IQA'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8033bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_qualities['CLIP-IQA-512']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_qualities['CLIP-IQA-32']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71e36771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash torch.Size([1281167])\n",
      "CLIP-IQA torch.Size([1281167, 1])\n",
      "CLIP-IQA-512 torch.Size([1281167])\n",
      "CLIP-IQA-256 torch.Size([1281167])\n",
      "CLIP-IQA-128 torch.Size([1281167])\n",
      "CLIP-IQA-64 torch.Size([1281167])\n",
      "CLIP-IQA-32 torch.Size([1281167])\n"
     ]
    }
   ],
   "source": [
    "for k in annotations_qualities.keys():\n",
    "  print(k, annotations_qualities[k].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871cfdeb",
   "metadata": {},
   "source": [
    "## Verify annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset, encoder, and network.\n",
    "print('Loading dataset...')\n",
    "dataset_obj = ambient_utils.dataset.SyntheticallyCorruptedImageFolderDataset(**dataset_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfcdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Annotations\n",
    "annotations = {}\n",
    "if annotations_qualities_path is not None:\n",
    "    annotations_qualities = load_stats(annotations_qualities_path)\n",
    "\n",
    "    ### Sigma min\n",
    "    global_qualities = annotations_qualities['CLIP-IQA'][:, 0]\n",
    "    assert len(global_qualities) == len(dataset_obj), f'Qualities ({len(global_qualities)}) and dataset_obj ({len(dataset_obj)}) must have equal lengths'\n",
    "\n",
    "    sorted_indices = torch.argsort(global_qualities, descending=True)\n",
    "    rank = torch.arange(len(global_qualities))[sorted_indices]\n",
    "    rank_threshold = int(len(global_qualities) * (1 - bad_data_percentage))\n",
    "    annotations_sigma_min = torch.where(rank < rank_threshold, 0.0, bad_data_sigma_min)\n",
    "\n",
    "    ### Sigma max\n",
    "    latents_receptive_field_to_sigma_max = {8:0.15, 16:0.25, 32:0.95}\n",
    "    annotations_sigma_max = torch.zeros(len(global_qualities))\n",
    "    if use_ambient_crops:\n",
    "        for latents_receptive_field in [8, 16, 32]:\n",
    "            pixel_receptive_field = 8 * latents_receptive_field\n",
    "            patch_qualities = annotations_qualities[f'CLIP-IQA-{pixel_receptive_field}']\n",
    "\n",
    "            sorted_indices = torch.argsort(patch_qualities, descending=True)\n",
    "            rank = torch.arange(len(patch_qualities))[sorted_indices]\n",
    "\n",
    "            rank_threshold = int(len(global_qualities) * (1 - bad_data_percentage))\n",
    "            good_data_sigma_max = latents_receptive_field_to_sigma_max[latents_receptive_field]\n",
    "\n",
    "            annotations_sigma_max = torch.where(rank < rank_threshold, good_data_sigma_max, annotations_sigma_max)\n",
    "\n",
    "    ### Annotations tuple\n",
    "    annotations = {dataset_obj._image_fnames[i]: (annotations_sigma_min[i], annotations_sigma_max[i]) for i in range(len(global_qualities))}\n",
    "else:\n",
    "    annotations = {dataset_obj._image_fnames[i]: (0.0, 0.0) for i in range(len(dataset_obj))}\n",
    "\n",
    "## Set dataset annotations\n",
    "dataset_obj.annotations = annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41cc012",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count number of samples with annotations zero\n",
    "count_zeros = sum(1 for value in annotations.values() if value[0] == 0.0)\n",
    "total_samples = len(annotations)\n",
    "percentage = (count_zeros / total_samples) * 100 if total_samples > 0 else 0\n",
    "print(f\"Number of samples with annotation 0.0: {count_zeros} out of {total_samples} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d683dd9",
   "metadata": {},
   "source": [
    "## Visualize top and bottom quality images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_display = 16\n",
    "global_qualities = annotations_qualities['CLIP-IQA'][:, 0]\n",
    "sorted_indices = torch.argsort(global_qualities, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display quality scores for top images\n",
    "print(\"Quality scores for top images:\")\n",
    "top_quality_image_indices = sorted_indices[:num_display]\n",
    "top_quality_scores = global_qualities[top_quality_image_indices]\n",
    "\n",
    "print(\" \".join([f\"Image {i + 1}: {score:.4f}\" for i, score in enumerate(top_quality_scores)]))\n",
    "\n",
    "images = torch.tensor([dataset_obj[i]['image'] for i in top_quality_image_indices])\n",
    "\n",
    "view_mosaic(images, nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quality scores for bottom images:\")\n",
    "bottom_quality_image_indices = sorted_indices[-num_display:]\n",
    "bottom_quality_scores = global_qualities[bottom_quality_image_indices]\n",
    "\n",
    "print(\" \".join([f\"Image {len(dataset_obj) - num_display + i + 1}: {score:.4f}\" for i, score in enumerate(bottom_quality_scores)]))\n",
    "\n",
    "images = torch.tensor([dataset_obj[i]['image'] for i in bottom_quality_image_indices])\n",
    "\n",
    "view_mosaic(images, nrow=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648790da",
   "metadata": {},
   "source": [
    "## Quality distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8bfd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with appropriate size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Prepare data\n",
    "data = {\n",
    "    'num_examples': np.arange(len(global_qualities)) / len(global_qualities),\n",
    "    'quality': global_qualities[sorted_indices].numpy(),\n",
    "}\n",
    "\n",
    "# Create a beautiful plot with custom styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lineplot(x='num_examples', y='quality', data=data, linewidth=2.5, color='#1f77b4')\n",
    "\n",
    "# Add title and improve labels\n",
    "plt.title('Image Quality Distribution', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Fraction of Dataset (sorted by quality)', fontsize=12)\n",
    "plt.ylabel('CLIP-IQA Quality Score', fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Improve tick labels\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
